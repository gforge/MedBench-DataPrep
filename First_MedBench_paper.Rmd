---
title: "First MedBench paper"
author: "Max Gordon"
date: "2024-09-30"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(tidyverse)
library(Gmisc)
library(jsonlite)
library(reticulate)
library(glue)
library(lme4)
library(broom.mixed)
library(kableExtra)

use_condaenv("base")
```

```{r, include=FALSE}
py_config()
```

```{r helper-functions}
output_mixed_model <- function(m, outcome, summary_fn = NULL) {
  # Get model results with confidence intervals
  model_summary <- broom.mixed::tidy(m, conf.int = TRUE)
  
  if (!is.null(summary_fn)) {
    model_summary <- summary_fn(model_summary)
  }
  
  # Add a column to indicate reference levels for categorical variables
  model_summary |> 
    filter(is.na(group)) |>   # Remove the random effect
    select(term, estimate, std.error, conf.low, conf.high) |> 
    rename(
      "Term" = term,
      "Estimate" = estimate,
      # "Std. Error" = std.error,
    ) |>
    mutate(across(where(is.numeric), \(x) htmlTable::txtRound(x, 2)),
           `95% CI` = glue::glue("{conf.low} to {conf.high}")) |> 
    select(-conf.low, -conf.high, -std.error) |>
    kable("html", caption = glue::glue("Mixed Model Results with Confidence Intervals for outcome {outcome}")) |> 
    kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
}

fix_labels <- function(x) {
  mutate(x,
         generatedBy = case_when(startsWith(generatedBy, "$") ~ str_replace(generatedBy, "^\\$", ""),
                                 TRUE ~ generatedBy),
         prompt_type = case_when(str_detect(generatedBy, "@basic") ~ "Simple",
                                 str_detect(generatedBy, "@decompos") ~ "Decompose",
                                 startsWith(generatedBy, "Human") ~ "Human"),
         chart_language = case_when(str_detect(chart_language, "original") ~ "English",
                                    TRUE ~ chart_language)) |> 
    select(-generatedBy) |> 
    relocate(prompt_type, .after = chart_language)
}

fix_factors <- function(x) {
  x |> 
    mutate(across(where(is.character), factor),
           prompt_type = relevel(prompt_type, ref = "Human"),
           chart_language = relevel(chart_language, ref = "English"),
           specialty = relevel(specialty, ref = "Orthopaedics"),
           generator = factor(prompt_type != "Human", labels = c("Human", "LLM")))
}
```

# Basics

```{r}
# Load the data
raw <- read_file("data/output/allData.json") |> fromJSON(simplifyVector = FALSE)
cases <- raw |> 
  map(\(x) x$charts |> 
        keep(\(c) length(c$summaries) > 1) |> 
        map(\(c) list(name = x$name,
                      specialty = x$specialty,
                      language = c$language,
                      summaries = c$summaries))) |> 
  discard(is_empty)

summaries <- cases |> 
  map(\(case) {
    map(case,
        \(chart) {
            map(chart$summaries,
                \(summary) list(name = chart$name,
                                specialty = chart$specialty,
                                chart_language = chart$language,
                                generatedBy = summary$generatedBy,
                                summary = summary$text)) |> 
            bind_rows()
        }) |> 
    bind_rows()
  }) |> 
  bind_rows()

reviews <- cases |> 
  map(\(case) {
    map(case,
        \(chart) {
            map(chart$summaries,
                \(summary) {
                  map(summary$reviews,
                      \(review) {
                        tibble(name = chart$name,
                               specialty = chart$specialty,
                               chart_language = chart$language,
                               generatedBy = summary$generatedBy,
                               reviewer = glue("{review$user$firstName} {review$user$lastName}")) |> 
                        bind_cols(as.data.frame(review$rating))
                      }) |> 
                    bind_rows() 
                  }) |> 
            bind_rows()
        }) |> 
    bind_rows()
  }) |> 
  bind_rows()
```

# Review ratings

```{r}
no_reviewers <- reviews |> count(reviewer) |> nrow()
no_english_reviews <- reviews |> filter(chart_language == "original") |> count(reviewer) |> nrow()
no_swedish_reviews <- reviews |> filter(chart_language == "Swedish") |> count(reviewer) |> nrow()

library(boot)

# Function to calculate mean and percentile-based CI
calculate_mean_and_ci <- function(data, indices) {
  d <- data[indices]
  return(mean(d))
}

# Bootstrap function
bootstrap_ci <- function(data, R = 1000, conf_level = 0.95) {
  boot_result <- boot(data, calculate_mean_and_ci, R = R)
  ci <- boot.ci(boot_result, type = "perc", conf = conf_level)
  list(
    mean = mean(data),
    lower_ci = ci$percent[4],
    upper_ci = ci$percent[5]
  ) |> 
    map(\(x) txtRound(x, digits = 1)) |> 
    with(glue::glue("{mean} (95% CI {lower_ci} - {upper_ci})"))
}

score <- list(overall = reviews,
              overall_english = reviews |> filter(chart_language == "original"),
              overall_swedish = reviews |> filter(chart_language == "Swedish")) |> 
  map(\(x) x |> pull(overall) |> bootstrap_ci())
```

We have `r nrow(reviews)` ratings from `r no_reviewers` reviewers. For English charts we have a total of `r no_english_reviews` reviewers, while for Swedish charts we have `r no_swedish_reviews` reviewers The overall rating is `r score$overall`, while the average rating for English charts is `r score$overall_english` and for Swedish charts `r score$overall_swedish`.

```{r prepped_data}
clean_reviews <- reviews |> 
  fix_labels() |> 
  select(-version, -ends_with("Comment"), -hallucinations) |> 
  fix_factors()

prepped_data <- clean_reviews |> 
  pivot_longer(cols = where(is.integer), names_to = "metric", values_to = "value") |>
  group_by(chart_language, prompt_type, metric) |>
  summarise(value = mean(value, na.rm =TRUE), .groups = "drop") |> 
  mutate(metric = case_when(metric == "hospitalCourse" ~ "Hospital course",
                            metric == "medicalHistory" ~ "Medical history",
                            metric == "followUp" ~ "Follow-up",
                            TRUE ~ Hmisc::capitalize(metric))) |> 
  pivot_wider(names_from = metric, values_from = value)

```

# Overall

For the overall rating none of the unusable were generated by hand and the majority of them were regarder as at least average or above. When modelling the score as a continuous variable using mixed regression we could see that the LLM estimates were on average Â½ point lower than the human ones and this was even more pronounced for the Swedish summaries.

```{r}
colors <- c("Human" = "#ff70a6", "Decompose" = "#ff9770", "Simple" = "#ffd670")
clean_reviews |> 
  select(chart_language, prompt_type, overall) |> 
  mutate(overall = factor(overall, 
                          labels = c("Unusable",
                                     "Subhuman",
                                     "Average",
                                     "Good",
                                     "Superhuman"))) |>
  ggplot(aes(x = overall, fill = prompt_type)) +
  facet_wrap(~chart_language, ncol = 1) +
  geom_bar(position = "dodge", alpha = 0.8, col = "black") +
  labs(x = "Chart Language",
       y = "Overall Rating") +
  theme_minimal() +
  theme(legend.position = "right",
        strip.text = element_text(size = 12)) +
  scale_fill_manual(values = colors,
                    name = "Prompt Type")
```

```{r}
reviews |> 
  fix_labels() |> 
  fix_factors() |> 
  select(name, specialty, generator, language = chart_language, reviewer, overall) |> 
  mutate(case_id = interaction(reviewer, name)) |> 
  lmer(overall ~ specialty + language * generator + (1 | reviewer), data = _) |> 
  output_mixed_model(outcome = "overall score",
                     summary_fn = \(x) mutate(x,
                                              term = case_when(
                                                term == "(Intercept)" ~ "(Intercept) [Reference: Orthopaedics, English, Human]",
                                                term == "specialtyOrthopaedics" ~ "Orthopaedics [vs. Medicine]",
                                                term == "specialtyMedicine" ~ "Medicine [vs. Orthopaedics]",
                                                term == "languageSwedish" ~ "Swedish [vs. English]",
                                                term == "prompt_typeSimple" ~ "Simple [vs. Human]",
                                                term == "prompt_typeDecompose" ~ "Decompose [vs. Human]",
                                                term == "generatorLLM" ~ "LLM [vs. Human]",
                                                term == "languageSwedish:generatorLLM" ~ "Swedish:LLM [vs. English:Human]",
                                                TRUE ~ term
                                              )))
```

## Metrics

For the metrics BLEU and ROUGE-L we found that the Swedish summaries dropped in quality compared to the English ones. The mixed effect regression showed that this effect was only significant for the language variable, the generated summaries did not vary in BLEU or ROUGE-L compared to the human ones if the summary was for an orthopedic or internal medicine case nor if we had used a simple approach or decomposed the summary.

```{python, echo=FALSE}
from dataclasses import dataclass, asdict
import pandas as pd
from rouge import Rouge
from sacrebleu.metrics import BLEU

# At first run do this:
# import nltk
# nltk.download('punkt')

df = r.summaries

rouge = Rouge(metrics=['rouge-l'],
              max_n=2,
              limit_length=True,
              length_limit=100,
              length_limit_type='words',
              apply_avg=False,
              apply_best=False,
              alpha=0.5,  # Default F1_score
              stemming=True)
blue = BLEU(effective_order=True)

# Define the data structure for each result row
@dataclass
class SummaryResult:
    specialty: str
    name: str
    chart_language: str
    generatedBy: str
    rouge_l_f: float
    rouge_l_p: float
    rouge_l_r: float
    bleu_score: float
    bleu_bp: float

# Function to calculate scores and return a dataclass instance
def calculate_scores_and_store_results(
    human_summary: str, 
    generated_summary: str, 
    specialty: str, 
    name: str, 
    language: str, 
    generatedBy: str
) -> SummaryResult:
    # Compute ROUGE scores
    score = rouge.get_scores(generated_summary, human_summary)
    rouge_l = score['rouge-l'][0]
    
    # Compute BLEU scores
    bleu_score = blue.sentence_score(generated_summary, [human_summary])

    # Return an instance of SummaryResult dataclass
    return SummaryResult(
        specialty=specialty,
        name=name,
        chart_language=language,
        generatedBy=generatedBy,
        rouge_l_f=rouge_l['f'][0],
        rouge_l_p=rouge_l['p'][0],
        rouge_l_r=rouge_l['r'][0],
        bleu_score=bleu_score.score,
        bleu_bp=bleu_score.bp,
    )

# Main function to process the DataFrame
def retrieve_summary_text_scores(df: pd.DataFrame) -> pd.DataFrame:
    results: list[SummaryResult] = []

    # Iterate using groupby to avoid unnecessary nested loops
    for (specialty, language, name), df_case in df.groupby(['specialty', 'chart_language', 'name']):
        # Get the human summary
        human_summary_row = df_case[df_case['generatedBy'] == 'Human']
        if human_summary_row.empty:
            continue  # Skip if no human summary is available

        human_summary = human_summary_row['summary'].values[0]

        # Get the generated summaries (all except 'Human')
        generated_summaries = df_case[df_case['generatedBy'] != 'Human']

        for idx, row in generated_summaries.iterrows():
            # Calculate scores and store the result as a dataclass instance
            result = calculate_scores_and_store_results(
                human_summary=human_summary, 
                generated_summary=row['summary'],
                specialty=specialty, 
                name=name, 
                language=language, 
                generatedBy=row['generatedBy']
            )
            results.append(result)

    # Convert list of dataclass instances to DataFrame
    return pd.DataFrame([asdict(result) for result in results])

# Convert results to DataFrame
results_df = retrieve_summary_text_scores(df)
```


```{r}
df <- py$results_df |> 
  fix_labels() |> 
  rename(language = chart_language)

df |> 
  group_by(language, prompt_type) |>
  summarise(across(starts_with("rouge") | starts_with("bleu"), mean),
            .groups = "drop") |> 
  pivot_longer(cols = -c(language, prompt_type), names_to = "metric", values_to = "value") |> 
  mutate(type = case_when(str_detect(metric, "rouge") ~ "ROUGE",
                          str_detect(metric, "bleu") ~ "BLEU"),
         value = txtRound(value, digits = 2)) |>
  filter(metric %in% c("rouge_l_f", "bleu_score")) |>
  arrange(language, type, prompt_type) |>
  tidyHtmlTable(value = value,
                rnames = prompt_type,
                rgroup = type,
                header = language,
                caption = "ROUGE-L F1 and BLEU scores for generated summaries per language and type")
```

```{r}
# Data preparation using the built-in pipe and across
df <- df |>
  mutate(across(c(specialty, language, prompt_type), as.factor),
         case_id = interaction(specialty, name))  # Create unique case_id for each specialty + name

# Fit a mixed model for rouge_l_f with random intercept for case_id
lmer(rouge_l_f ~ specialty + language + prompt_type + (1 | case_id), data = df) |> 
  output_mixed_model("ROUGE-L F1",
                     summary_fn = \(x) mutate(x,
                                              term = case_when(
                                                term == "(Intercept)" ~ "(Intercept) [Reference: Medicine, English, Decompose]",
                                                term == "specialtyOrthopaedics" ~ "Orthopaedics [vs. Medicine]",
                                                term == "languageSwedish" ~ "Swedish [vs. English]",
                                                term == "prompt_typeSimple" ~ "Simple [vs. Decompose]",
                                                TRUE ~ term
                                              )))

# Fit a mixed model for bleu_score with random intercept for case_id
lmer(bleu_score ~ specialty + language + prompt_type + (1 | case_id), data = df) |> 
  output_mixed_model("BLEU",
                     summary_fn = \(x) mutate(x,
                                              term = case_when(
                                                term == "(Intercept)" ~ "(Intercept) [Reference: Medicine, English, Decompose]",
                                                term == "specialtyOrthopaedics" ~ "Orthopaedics [vs. Medicine]",
                                                term == "languageSwedish" ~ "Swedish [vs. English]",
                                                term == "prompt_typeSimple" ~ "Simple [vs. Decompose]",
                                                TRUE ~ term
                                              )))
```

# Subelements

Similar patterns were seen when looking into the specific dimensions where follow-up was worse throughout and the Swedish cases had a worse performance for the majority of the outcomes.

```{r radar_data}
max_min <- prepped_data |> 
  select(where(is.numeric)) |> 
  apply(2, function(x) c(4, 1)) |> 
  as.data.frame()

# Combine max, min, and actual data for radar chart
radar_data <- bind_rows(max_min, 
                        prepped_data)
```

```{r}
library(fmsb)

plot_radar <- function(x, language) {
  data <- x |> 
    filter(chart_language == language | is.na(chart_language)) |> 
    mutate(order = case_when(is.na(prompt_type) ~ 1,
                             prompt_type == "Human" ~ 2,
                             TRUE ~ 10)) |> 
    arrange(order) |>
    select(-order, -chart_language)
    
  legends <- data$prompt_type |> 
    na.omit() |> 
    unique()
  
  main_colors <- RColorBrewer::brewer.pal(8, "Set2")[1:length(legends)] |>
    col2rgb() |> 
    t()
  colors_border <- rgb(main_colors, alpha = 0.9*255, maxColorValue = 255)
  colors_in <- rgb(main_colors, alpha = 0.4*255, maxColorValue = 255)
  
  data |> 
    select(-prompt_type) |> 
    radarchart(axistype = 1,
               maxmin = TRUE,
               # Customize the polygon colors
               pcol = colors_border,
               pfcol = colors_in,
               plwd = 4,
               # Customize the grid colors
               cglcol = "grey", 
               cglty = 1, 
               axislabcol = "grey", 
               caxislabels = seq(0, 5, 1), 
               cglwd = 0.8,
               # Customize labels
               vlcex = 0.8,
               title = language,
    )
  
  par(xpd = TRUE)
  legend(c(0.5,1), 
         legend = legends, 
         col = colors_border,
         pch = 20, 
         bty = "n",
         cex = 0.8)
}

per_language_radar <- function(x) {
  languages <- x$chart_language |> 
    na.omit() |> 
    unique()
  
  # Split the screen into multiple columns (one per language)
  org_par <- par(mfrow = c(1, length(languages)),
                 mar = rep(0.8, 4),  # Outer margins for the overall plot space
                 oma = rep(0.2, 4))  # Outer margins for the overall plot space
  on.exit(par(org_par))  # Reset to original layout
  
  for (language in languages) {
    plot_radar(x, language)
  }
}
```

## Medical accuracy

```{r, fig.height=6, fig.width=16}
radar_data |> 
  select(chart_language, 
         prompt_type, 
         Diagnosis,
         `Medical history`,
         `Hospital course`,
         `Follow-up`) |> 
  per_language_radar()
```

## Linguistic qualities

```{r, fig.height=6, fig.width=16}
radar_data |> 
  select(chart_language, 
         prompt_type, 
         Conciseness,
         Completeness,
         Language,
         Clarity) |> 
  per_language_radar()
```

# Hallucinations

```{r}
reviews |> 
  fix_labels() |> 
  select(prompt_type, hallucinations) |> 
  mutate(hallucinations = factor(hallucinations,
                                 levels = 0:3,
                                 labels = c("None", 
                                            "1 to 2",
                                            "3 to 4",
                                            "More than 4"))) |> 
  ggplot(aes(x = hallucinations, fill = prompt_type)) +
  geom_bar(position = "dodge", color = "black", alpha = 0.8) +
  scale_fill_manual(values = colors,
                    name = "Prompt Type") +
  labs(title = "Hallucinations in generated summaries",
       x = "Number of hallucinations",
       y = "Count") +
  theme_minimal() +
  theme(legend.position = "right")
```
```{r}
reviews |> 
  fix_labels() |> 
  select(prompt_type, specialty, name, hallucinationsComment) |> 
  filter(hallucinationsComment != "") |> 
  mutate(header = "Comment",
         name = interaction(specialty, name, sep = " - "),
         rnames_unique = 1:n()) |> 
  arrange(prompt_type, name)  |> 
  htmlTable::addHtmlTableStyle(col.rgroup = c("none", "#F7F7F7")) |> 
  tidyHtmlTable(tspanner = prompt_type,
                value = hallucinationsComment,
                header = header,
                rnames_unique = rnames_unique,
                align = "l")
```

# General comments

```{r}
reviews |> 
  fix_labels() |> 
  select(prompt_type, specialty, name, overallComment) |> 
  filter(overallComment != "") |> 
  mutate(header = "Comment",
         name = interaction(specialty, name, sep = " - "),
         rnames_unique = 1:n()) |> 
  arrange(prompt_type, name)  |> 
  htmlTable::addHtmlTableStyle(col.rgroup = c("none", "#F7F7F7")) |> 
  tidyHtmlTable(tspanner = prompt_type,
                value = overallComment,
                header = header,
                rnames_unique = rnames_unique,
                align = "l")

```

