---
title: "First MedBench paper"
author: "Max Gordon"
date: "2024-09-30"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(tidyverse)
library(Gmisc)
library(jsonlite)
library(reticulate)
library(glue)
library(lme4)
library(broom.mixed)
library(kableExtra)
library(boot)
library(fmsb)

use_condaenv("base")
```

```{python python-versions}
import sacrebleu
import rouge

sacrebleu_version = sacrebleu.__version__
rouge_version = rouge.__version__
```

```{r version-output}
versions <- list(R = paste(version$major, version$minor, sep="."),
                 lme4 = packageVersion("lme4"),
                 broom.mixed = packageVersion("broom.mixed"),
                 boot = packageVersion("boot"),
                 fmsb = packageVersion("fmsb"),
                 ggplot2 = packageVersion("ggplot2"),
                 python = py_config()$version_string |> str_replace("\\|.+", ""),
                 sacrebleu = py$sacrebleu_version,
                 rouge = py$rouge_version) |> 
  map(as.character) |> 
  map(str_trim)
```

# Statistics

Statistical analyses were conducted using R version `r versions$R` and Python `r versions$python`. BLEU scores were computed using the sacrebleu package (version `r versions$sacrebleu`), and ROUGE-L scores were calculated with the rouge package (version `r versions$rouge`) in Python.

Due to the hierarchical nature of the data we used linear mixed effects models. The mixed-effects models were fitted using the lme4 package (version `r versions$lme4`) and broom.mixed (version `r versions$broom.mixed`) in R. For the BLEU/ROUGE-L metrics we used specialty combined with case id as random effect, while for manual evaluation output we used reviewer id as the mixed effect variable. Model estimates, standard errors, and 95% confidence intervals were calculated for the fixed effects. Statistical significance was assessed at a threshold of p < 0.05.

Bootstrap methods with 1,000 resamples were employed using the boot package (version `r versions$boot`) in R to calculate percentile-based 95% confidence intervals for mean scores. Basic statistical summaries derived from the bootstrapping were presented prior to the advanced analyses to provide an overview of the data distributions. Data visualization was performed using ggplot2 (version `r versions$ggplot2`) and fmsb (version `r versions$fmsb`) packages in R.

```{r helper-functions}
output_mixed_model <- function(m, outcome, summary_fn = NULL, digits = 2) {
  # Get model results with confidence intervals
  model_summary <- broom.mixed::tidy(m, conf.int = TRUE)
  
  if (!is.null(summary_fn)) {
    model_summary <- summary_fn(model_summary)
  }
  
  # Add a column to indicate reference levels for categorical variables
  model_summary |> 
    filter(is.na(group)) |>   # Remove the random effect
    select(term, estimate, std.error, conf.low, conf.high) |> 
    rename(
      "Term" = term,
      "Estimate" = estimate,
      # "Std. Error" = std.error,
    ) |>
    mutate(across(where(is.numeric), \(x) htmlTable::txtRound(x, digits = digits)),
           `95% CI` = glue::glue("{conf.low} to {conf.high}")) |> 
    select(-conf.low, -conf.high, -std.error) |>
    structure(outcome = outcome,
              class = c("mixed_table", "data.frame"))
}

print.mixed_table <- function(x, ...) {
  caption <- glue::glue("Mixed Model Results with Confidence Intervals for outcome {outcome}",
                        outcome = attr(x, "outcome"))
  x |> 
    kable("html", caption = caption, escape = FALSE) |>
    kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
}

toString.mixed_table <- function(x, term_id, ...) {
  row <- x |> filter(Term == term_id)
  if (nrow(row) == 0) {
    stop("Row not found: ", term_id,
         " available rows: \n - ", 
         paste(x$Term, collapse = "\n - "))
  }
  
  if (nrow(row) > 1) {
    stop("Multiple rows found: ", term_id)
  }
  
  glue::glue("{row$Estimate} (95% CI {row$`95% CI`})")
}

fix_labels <- function(x) {
  mutate(x,
         generatedBy = case_when(startsWith(generatedBy, "$") ~ str_replace(generatedBy, "^\\$", ""),
                                 TRUE ~ generatedBy),
         prompt_type = case_when(str_detect(generatedBy, "@basic") ~ "Singel",
                                 str_detect(generatedBy, "@decompos") ~ "Multistep",
                                 startsWith(generatedBy, "Human") ~ "Human"),
         chart_language = case_when(str_detect(chart_language, "original") ~ "English",
                                    TRUE ~ chart_language)) |> 
    select(-generatedBy) |> 
    relocate(prompt_type, .after = chart_language)
}

fix_factors <- function(x) {
  x |> 
    mutate(across(where(is.character), factor),
           prompt_type = relevel(prompt_type, ref = "Human"),
           chart_language = relevel(chart_language, ref = "English"),
           specialty = relevel(specialty, ref = "Orthopaedics"),
           generator = factor(prompt_type != "Human", labels = c("Human", "LLM")))
}
```

```{r prep_data}
# Load the data
raw <- read_file("data/output/allData.json") |> fromJSON(simplifyVector = FALSE)
cases <- raw |> 
  map(\(x) x$charts |> 
        keep(\(c) length(c$summaries) > 1) |> 
        map(\(c) list(name = x$name,
                      specialty = x$specialty,
                      language = c$language,
                      summaries = c$summaries))) |> 
  discard(is_empty)

summaries <- cases |> 
  map(\(case) {
    map(case,
        \(chart) {
            map(chart$summaries,
                \(summary) list(name = chart$name,
                                specialty = chart$specialty,
                                chart_language = chart$language,
                                generatedBy = summary$generatedBy,
                                summary = summary$text)) |> 
            bind_rows()
        }) |> 
    bind_rows()
  }) |> 
  bind_rows()

reviews <- cases |> 
  map(\(case) {
    map(case,
        \(chart) {
            map(chart$summaries,
                \(summary) {
                  map(summary$reviews,
                      \(review) {
                        tibble(name = chart$name,
                               specialty = chart$specialty,
                               chart_language = chart$language,
                               generatedBy = summary$generatedBy,
                               reviewer = glue("{review$user$firstName} {review$user$lastName}")) |> 
                        bind_cols(as.data.frame(review$rating))
                      }) |> 
                    bind_rows() 
                  }) |> 
            bind_rows()
        }) |> 
    bind_rows()
  }) |> 
  bind_rows()

clean_reviews <- reviews |> 
  fix_labels() |> 
  select(-version, -ends_with("Comment"), -hallucinations) |> 
  fix_factors()
```

```{r base-stats}
# Function to calculate mean and percentile-based CI
calculate_mean_and_ci <- function(data, indices) {
  d <- data[indices]
  return(mean(d))
}

# Bootstrap function
bootstrap_ci <- function(data, R = 1000, conf_level = 0.95) {
  boot_result <- boot(data, calculate_mean_and_ci, R = R)
  ci <- boot.ci(boot_result, type = "perc", conf = conf_level)
  list(
    mean = mean(data),
    lower_ci = ci$percent[4],
    upper_ci = ci$percent[5]
  ) |> 
    map(\(x) txtRound(x, digits = 1)) |> 
    with(glue::glue("{mean} (95% CI {lower_ci} - {upper_ci})"))
}

base_stats <- list(cases = raw |> length(),
                   cases_with_summaries = summaries |> distinct(name, specialty, chart_language) |> nrow(),
                   unique_cases_with_summaries = summaries |> distinct(name, specialty) |> nrow(),
                   reviewers = reviews |> count(reviewer) |> nrow(),
                   english_reviews = reviews |> filter(chart_language == "original") |> count(reviewer) |> nrow(),
                   swedish_reviews = reviews |> filter(chart_language == "Swedish") |> count(reviewer) |> nrow()) |> 
  set_names(\(x) paste0("no_", x)) |> 
  append(list(overall = reviews,
              overall_english = reviews |> filter(chart_language == "original"),
              overall_swedish = reviews |> filter(chart_language == "Swedish")) |> 
           map(\(x) x |> pull(overall) |> bootstrap_ci()) |> 
           set_names(\(x) paste0("score_", x)))
```

# Results

For the `r base_stats$no_cases` cases, we have reviewed summaries for `r base_stats$no_cases_with_summaries` cases (`r base_stats$no_unique_cases_with_summaries` unique cases). The cases were reviewed by `r base_stats$no_reviewers` reviewers, amounting to a total of `r nrow(reviews)` ratings. For English charts we have a total of `r base_stats$no_english_reviews` reviewers, while for Swedish charts we have `r base_stats$no_swedish_reviews` reviewers The overall rating is `r base_stats$score_overall`, while the average rating for English charts is `r base_stats$score_overall_english` and for Swedish charts `r base_stats$score_overall_swedish`.


```{python python-metrics, echo=FALSE}
from dataclasses import dataclass, asdict
import pandas as pd
from rouge import Rouge
from sacrebleu.metrics import BLEU

# At first run do this:
# import nltk
# nltk.download('punkt')

df = r.summaries

rouge = Rouge(metrics=['rouge-l'],
              max_n=2,
              limit_length=True,
              length_limit=100,
              length_limit_type='words',
              apply_avg=False,
              apply_best=False,
              alpha=0.5,  # Default F1_score
              stemming=True)
blue = BLEU(effective_order=True)

# Define the data structure for each result row
@dataclass
class SummaryResult:
    specialty: str
    name: str
    chart_language: str
    generatedBy: str
    rouge_l_f: float
    rouge_l_p: float
    rouge_l_r: float
    bleu_score: float
    bleu_bp: float

# Function to calculate scores and return a dataclass instance
def calculate_scores_and_store_results(
    human_summary: str, 
    generated_summary: str, 
    specialty: str, 
    name: str, 
    language: str, 
    generatedBy: str
) -> SummaryResult:
    # Compute ROUGE scores
    score = rouge.get_scores(generated_summary, human_summary)
    rouge_l = score['rouge-l'][0]
    
    # Compute BLEU scores
    bleu_score = blue.sentence_score(generated_summary, [human_summary])

    # Return an instance of SummaryResult dataclass
    return SummaryResult(
        specialty=specialty,
        name=name,
        chart_language=language,
        generatedBy=generatedBy,
        rouge_l_f=rouge_l['f'][0],
        rouge_l_p=rouge_l['p'][0],
        rouge_l_r=rouge_l['r'][0],
        bleu_score=bleu_score.score,
        bleu_bp=bleu_score.bp,
    )

# Main function to process the DataFrame
def retrieve_summary_text_scores(df: pd.DataFrame) -> pd.DataFrame:
    results: list[SummaryResult] = []

    # Iterate using groupby to avoid unnecessary nested loops
    for (specialty, language, name), df_case in df.groupby(['specialty', 'chart_language', 'name']):
        # Get the human summary
        human_summary_row = df_case[df_case['generatedBy'] == 'Human']
        if human_summary_row.empty:
            continue  # Skip if no human summary is available

        human_summary = human_summary_row['summary'].values[0]

        # Get the generated summaries (all except 'Human')
        generated_summaries = df_case[df_case['generatedBy'] != 'Human']

        for idx, row in generated_summaries.iterrows():
            # Calculate scores and store the result as a dataclass instance
            result = calculate_scores_and_store_results(
                human_summary=human_summary, 
                generated_summary=row['summary'],
                specialty=specialty, 
                name=name, 
                language=language, 
                generatedBy=row['generatedBy']
            )
            results.append(result)

    # Convert list of dataclass instances to DataFrame
    return pd.DataFrame([asdict(result) for result in results])

# Convert results to DataFrame
results_df = retrieve_summary_text_scores(df)
```

```{r metrics-regression}
df <- py$results_df |> 
  fix_labels() |> 
  rename(language = chart_language)

# Data preparation using the built-in pipe and across
regression_df <- df |>
  mutate(across(c(specialty, language, prompt_type), as.factor),
         case_id = interaction(specialty, name))  # Create unique case_id for each specialty + name

# Fit a mixed model for rouge_l_f with random intercept for case_id
metrics_regression_outputs <- local({
  fn <- \(x) mutate(x,
                    term = case_when(
                      term == "(Intercept)" ~ "(Intercept) [Reference: Medicine, English, Multistep]",
                      term == "specialtyOrthopaedics" ~ "Orthopaedics [vs. Medicine]",
                      term == "languageSwedish" ~ "Swedish [vs. English]",
                      term == "prompt_typeSingel" ~ "Singel [vs. Multistep]",
                      TRUE ~ term
                    ))
  list(
    rouge = lmer(rouge_l_f ~ specialty + language + prompt_type + (1 | case_id), data = regression_df) |> 
      output_mixed_model("ROUGE-L F1",
                         summary_fn = fn,
                         digits = 2),
    bleu = lmer(bleu_score ~ specialty + language + prompt_type + (1 | case_id), data = regression_df) |> 
      output_mixed_model("BLEU",
                         summary_fn = fn,
                         digits = 1))
})
```


```{r overall_output_regression}
overall_output <- reviews |> 
  fix_labels() |> 
  fix_factors() |> 
  select(name, specialty, generator, language = chart_language, reviewer, overall) |> 
  mutate(case_id = interaction(reviewer, name)) |> 
  lmer(overall ~ specialty + language * generator + (1 | reviewer), data = _) |> 
  output_mixed_model(outcome = "overall score",
                     summary_fn = \(x) mutate(x,
                                              term = case_when(
                                                term == "(Intercept)" ~ "(Intercept) [Reference: Orthopaedics, English, Human]",
                                                term == "specialtyOrthopaedics" ~ "Orthopaedics [vs. Medicine]",
                                                term == "specialtyMedicine" ~ "Medicine [vs. Orthopaedics]",
                                                term == "languageSwedish" ~ "Swedish [vs. English]",
                                                term == "prompt_typeSingel" ~ "Singel [vs. Human]",
                                                term == "prompt_typeMultistep" ~ "Multistep [vs. Human]",
                                                term == "generatorLLM" ~ "LLM [vs. Human]",
                                                term == "languageSwedish:generatorLLM" ~ "Interaction: Swedish & LLM",
                                                TRUE ~ term
                                              )))
```

For the overall rating of the summaries, none of the summaries deemed unusable were generated by humans and the majority of them were regarded as at least average or above. When modelling the score as a continuous variable using mixed regression we found that the LLM estimates were on average lower than the human ones, `r toString(overall_output, "LLM [vs. Human]")` and this was even more pronounced for the Swedish summaries, `r toString(overall_output, "Interaction: Swedish & LLM")`. Similarly the BLEU and ROUGE-L metrics dropped when comparing the LLM to the human summaries, `r toString(metrics_regression_outputs$bleu, "Swedish [vs. English]")` and `r toString(metrics_regression_outputs$rouge, "Swedish [vs. English]")` respectively.

```{r}
colors <- c("Human" = "#7fbf7b", "Multistep" = "#af8dc3", "Singel" = "#e7d4e8")
clean_reviews |> 
  select(chart_language, prompt_type, overall) |> 
  mutate(overall = factor(overall, 
                          labels = c("Unusable",
                                     "Subhuman",
                                     "Average",
                                     "Good",
                                     "Superhuman"))) |>
  ggplot(aes(x = overall, fill = prompt_type)) +
  facet_wrap(~chart_language, ncol = 1) +
  geom_bar(position = "dodge", alpha = 0.8, col = "black") +
  labs(x = "Chart Language",
       y = "Overall Rating") +
  theme_minimal() +
  theme(legend.position = "right",
        strip.text = element_text(size = 12)) +
  scale_fill_manual(values = colors,
                    name = "Prompt Type")
```

```{r overall_mixed_model_output}
print(overall_output)
```

```{r metrics_mixed_model_output}
print(metrics_regression_outputs$rouge)
print(metrics_regression_outputs$bleu)
```

```{r}
df |> 
  group_by(language, prompt_type) |>
  summarise(across(starts_with("rouge") | starts_with("bleu"), mean),
            .groups = "drop") |> 
  pivot_longer(cols = -c(language, prompt_type), names_to = "metric", values_to = "value") |> 
  mutate(type = case_when(str_detect(metric, "rouge") ~ "ROUGE",
                          str_detect(metric, "bleu") ~ "BLEU"),
         value = txtRound(value, digits = 2)) |>
  filter(metric %in% c("rouge_l_f", "bleu_score")) |>
  arrange(language, type, prompt_type) |>
  tidyHtmlTable(value = value,
                rnames = prompt_type,
                rgroup = type,
                header = language,
                caption = "ROUGE-L F1 and BLEU scores for generated summaries per language and type")
```

For the specific domains we found that follow-up in general was worse for the LLM-generated models. Both human and LLM generated poorly in regards to medications, although for the human notes this was frequently not present due to it not being a part of a standard orthopedic discharge note. In general, most LLM summaries performed worse in Swedish and followed the same pattern as indicated by the overall score.

```{r}
prepped_data <- clean_reviews |> 
  pivot_longer(cols = where(is.integer), names_to = "metric", values_to = "value") |>
  group_by(chart_language, prompt_type, metric) |>
  summarise(value = mean(value, na.rm =TRUE), .groups = "drop") |> 
  mutate(metric = case_when(metric == "hospitalCourse" ~ "Hospital course",
                            metric == "medicalHistory" ~ "Medical history",
                            metric == "followUp" ~ "Follow-up",
                            TRUE ~ Hmisc::capitalize(metric))) |> 
  pivot_wider(names_from = metric, values_from = value)
```

```{r radar_data}
max_min <- prepped_data |> 
  select(where(is.numeric)) |> 
  apply(2, function(x) c(4, 1)) |> 
  as.data.frame()

# Combine max, min, and actual data for radar chart
radar_data <- bind_rows(max_min, 
                        prepped_data)
```

```{r}
library(fmsb)

plot_radar <- function(x, language) {
  data <- x |> 
    filter(chart_language == language | is.na(chart_language)) |> 
    mutate(order = case_when(is.na(prompt_type) ~ 1,
                             prompt_type == "Human" ~ 2,
                             TRUE ~ 10)) |> 
    arrange(order) |>
    select(-order, -chart_language)
    
  legends <- data$prompt_type |> 
    na.omit() |> 
    unique()
  
  main_colors <- RColorBrewer::brewer.pal(8, "Set2")[1:length(legends)] |>
    col2rgb() |> 
    t()
  colors_border <- rgb(main_colors, alpha = 0.9*255, maxColorValue = 255)
  colors_in <- rgb(main_colors, alpha = 0.4*255, maxColorValue = 255)
  
  data |> 
    select(-prompt_type) |> 
    radarchart(axistype = 1,
               maxmin = TRUE,
               # Customize the polygon colors
               pcol = colors_border,
               pfcol = colors_in,
               plwd = 4,
               # Customize the grid colors
               cglcol = "grey", 
               cglty = 1, 
               axislabcol = "grey", 
               caxislabels = seq(0, 5, 1), 
               cglwd = 0.8,
               # Customize labels
               vlcex = 0.8,
               title = language,
    )
  
  par(xpd = TRUE)
  legend(c(0.5,1), 
         legend = legends, 
         col = colors_border,
         pch = 20, 
         bty = "n",
         cex = 0.8)
}

per_language_radar <- function(x) {
  languages <- x$chart_language |> 
    na.omit() |> 
    unique()
  
  # Split the screen into multiple columns (one per language)
  org_par <- par(mfrow = c(1, length(languages)),
                 mar = rep(0.8, 4),  # Outer margins for the overall plot space
                 oma = rep(0.2, 4))  # Outer margins for the overall plot space
  on.exit(par(org_par))  # Reset to original layout
  
  for (language in languages) {
    plot_radar(x, language)
  }
}
```

## Medical accuracy

```{r, fig.height=6, fig.width=16}
radar_data |> 
  select(chart_language, 
         prompt_type, 
         Diagnosis,
         `Medical history`,
         `Hospital course`,
         `Follow-up`) |> 
  per_language_radar()
```

## Linguistic qualities

```{r, fig.height=6, fig.width=16}
radar_data |> 
  select(chart_language, 
         prompt_type, 
         Conciseness,
         Completeness,
         Language,
         Clarity) |> 
  per_language_radar()
```

# Hallucinations

```{r}
reviews |> 
  fix_labels() |> 
  select(prompt_type, hallucinations) |> 
  mutate(hallucinations = factor(hallucinations,
                                 levels = 0:3,
                                 labels = c("None", 
                                            "1 to 2",
                                            "3 to 4",
                                            "More than 4"))) |> 
  ggplot(aes(x = hallucinations, fill = prompt_type)) +
  geom_bar(position = "dodge", color = "black", alpha = 0.8) +
  scale_fill_manual(values = colors,
                    name = "Prompt Type") +
  labs(title = "Hallucinations in generated summaries",
       x = "Number of hallucinations",
       y = "Count") +
  theme_minimal() +
  theme(legend.position = "right")
```
```{r}
reviews |> 
  fix_labels() |> 
  select(prompt_type, specialty, name, hallucinationsComment) |> 
  filter(hallucinationsComment != "") |> 
  mutate(header = "Comment",
         name = interaction(specialty, name, sep = " - "),
         rnames_unique = 1:n()) |> 
  arrange(prompt_type, name)  |> 
  htmlTable::addHtmlTableStyle(col.rgroup = c("none", "#F7F7F7")) |> 
  tidyHtmlTable(tspanner = prompt_type,
                value = hallucinationsComment,
                header = header,
                rnames_unique = rnames_unique,
                align = "l")
```

# General comments

```{r}
reviews |> 
  fix_labels() |> 
  select(prompt_type, specialty, name, overallComment) |> 
  filter(overallComment != "") |> 
  mutate(header = "Comment",
         name = interaction(specialty, name, sep = " - "),
         rnames_unique = 1:n()) |> 
  arrange(prompt_type, name)  |> 
  htmlTable::addHtmlTableStyle(col.rgroup = c("none", "#F7F7F7")) |> 
  tidyHtmlTable(tspanner = prompt_type,
                value = overallComment,
                header = header,
                rnames_unique = rnames_unique,
                align = "l")

```

# Author contributions

```{r}
reviews |> 
  fix_labels() |> 
  group_by(reviewer) |>
  summarise(n = n(),
            .groups = "drop") |>
  arrange(desc(n)) |>
  htmlTable::htmlTable()
```

