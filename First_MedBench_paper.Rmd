---
title: "First MedBench paper"
author: "Max Gordon"
date: "2024-09-30"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.dpi=300)
library(tidyverse)
library(Gmisc)
library(jsonlite)
library(reticulate)
library(glue)
library(lme4)
library(broom.mixed)
library(kableExtra)
library(boot)
library(fmsb)
library(ggpubr)

use_condaenv("base")
```

```{python python-versions}
import sacrebleu
import rouge

sacrebleu_version = sacrebleu.__version__
rouge_version = rouge.__version__
```

```{r version-output}
versions <- list(R = paste(version$major, version$minor, sep="."),
                 lme4 = packageVersion("lme4"),
                 broom.mixed = packageVersion("broom.mixed"),
                 boot = packageVersion("boot"),
                 fmsb = packageVersion("fmsb"),
                 ggpubr = packageVersion("ggpubr"),
                 ggplot2 = packageVersion("ggplot2"),
                 python = py_config()$version_string |> str_replace("\\|.+", ""),
                 sacrebleu = py$sacrebleu_version,
                 rouge = py$rouge_version) |> 
  map(as.character) |> 
  map(str_trim)
```

```{r helper-functions}
output_mixed_model <- function(m, outcome, summary_fn = NULL, digits = 2) {
  # Get model results with confidence intervals
  model_summary <- broom.mixed::tidy(m, conf.int = TRUE)
  
  if (!is.null(summary_fn)) {
    model_summary <- summary_fn(model_summary)
  }
  
  # Add a column to indicate reference levels for categorical variables
  model_summary |> 
    filter(is.na(group)) |>   # Remove the random effect
    select(term, estimate, std.error, conf.low, conf.high) |> 
    rename(
      "Term" = term,
      "Estimate" = estimate,
      # "Std. Error" = std.error,
    ) |>
    mutate(across(where(is.numeric), \(x) htmlTable::txtRound(x, digits = digits)),
           `95% CI` = glue::glue("{conf.low} to {conf.high}")) |> 
    select(-conf.low, -conf.high, -std.error) |>
    structure(outcome = outcome,
              class = c("mixed_table", "data.frame"))
}

print.mixed_table <- function(x, ...) {
  caption <- glue::glue("Mixed Model Results with Confidence Intervals for outcome {outcome}",
                        outcome = attr(x, "outcome"))
  x |> 
    kable("html", caption = caption, escape = FALSE) |>
    kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
}

toString.mixed_table <- function(x, term_id, invert = FALSE, unit = NULL, ...) {
  row <- x |> filter(Term == term_id)
  if (nrow(row) == 0) {
    stop("Row not found: ", term_id,
         " available rows: \n - ", 
         paste(x$Term, collapse = "\n - "))
  }
  
  if (nrow(row) > 1) {
    stop("Multiple rows found: ", term_id)
  }
  
  estimate <- row$Estimate
  if (invert) {
    str_inverter <- function(x) {
      x <- str_trim(x)
      if (str_detect(x, "^-")) {
        str_replace(x, "^-", "")
      } else {
        paste0("-", x)
      }
    }
    estimate <- str_inverter(estimate)
  }
  
  glue::glue("{estimate}{unit_txt} (95% CI {interval})",
             unit_txt = ifelse(is.null(unit), "", glue::glue(" {unit}")),
             estimate = estimate,
             interval = row$`95% CI`)
}

fix_labels <- function(x) {
  mutate(x,
         generatedBy = case_when(startsWith(generatedBy, "$") ~ str_replace(generatedBy, "^\\$", ""),
                                 TRUE ~ generatedBy),
         prompt_type = case_when(str_detect(generatedBy, "@basic") ~ "Singel",
                                 str_detect(generatedBy, "@decompos") ~ "Multistep",
                                 startsWith(generatedBy, "Human") ~ "Human"),
         chart_language = case_when(str_detect(chart_language, "original") ~ "English",
                                    TRUE ~ chart_language)) |> 
    select(-generatedBy) |> 
    relocate(prompt_type, .after = chart_language)
}

fix_factors <- function(x) {
  x |> 
    mutate(across(where(is.character), factor),
           prompt_type = relevel(prompt_type, ref = "Human"),
           chart_language = relevel(chart_language, ref = "English"),
           specialty = relevel(specialty, ref = "Orthopaedics"),
           generator = factor(prompt_type != "Human", labels = c("Human", "LLM")))
}
```

```{r prep_data}
# Load the data
raw <- read_file("data/output/allData.json") |> fromJSON(simplifyVector = FALSE)

raw_with_datasets <- raw |> 
  map(\(x) x$charts |> 
        map(function(chart) {
          list(name = x$name,
               specialty = x$specialty,
               language = chart$language,
               status = chart$status,
               createdBy = chart$createdBy,
               summaries = chart$summaries,
               notes = bind_rows(chart$notes),
               lab_values = bind_rows(chart$lab),
               medications = bind_rows(chart$medications)) |> 
            map(\(x) {
              if (!is_tibble(x)) {
                return(x)
              }
              
              x |> 
                select(!one_of("__typename", "id", "chartId"))
              
            })
        }))

cases <- raw_with_datasets |> 
  map(\(x) x |> 
        keep(\(chart) length(chart$summaries) > 1) |> 
        map(\(chart) list(name = chart$name,
                          specialty = chart$specialty,
                          language = chart$language,
                          summaries = chart$summaries))) |> 
  discard(is_empty)

summaries <- cases |> 
  map(\(case) {
    map(case,
        \(chart) {
            map(chart$summaries,
                \(summary) list(name = chart$name,
                                specialty = chart$specialty,
                                chart_language = chart$language,
                                generatedBy = summary$generatedBy,
                                summary = summary$text)) |> 
            bind_rows()
        }) |> 
    bind_rows()
  }) |> 
  bind_rows()

reviews <- cases |> 
  map(\(case) {
    map(case,
        \(chart) {
            map(chart$summaries,
                \(summary) {
                  map(summary$reviews,
                      \(review) {
                        tibble(name = chart$name,
                               specialty = chart$specialty,
                               chart_language = chart$language,
                               generatedBy = summary$generatedBy,
                               reviewer = glue("{review$user$firstName} {review$user$lastName}")) |> 
                        bind_cols(as.data.frame(review$rating))
                      }) |> 
                    bind_rows() 
                  }) |> 
            bind_rows()
        }) |> 
    bind_rows()
  }) |> 
  bind_rows()

clean_reviews <- reviews |> 
  fix_labels() |> 
  select(-version, -ends_with("Comment"), -hallucinations) |> 
  fix_factors()
```

```{r base-stats}
# Function to calculate mean and percentile-based CI
calculate_mean_and_ci <- function(data, indices) {
  d <- data[indices]
  return(mean(d))
}

# Bootstrap function
bootstrap_ci <- function(data, R = 1000, conf_level = 0.95) {
  boot_result <- boot(data, calculate_mean_and_ci, R = R)
  ci <- boot.ci(boot_result, type = "perc", conf = conf_level)
  list(
    mean = mean(data),
    lower_ci = ci$percent[4],
    upper_ci = ci$percent[5]
  ) |> 
    map(\(x) txtRound(x, digits = 1)) |> 
    with(glue::glue("{mean} (95% CI {lower_ci} - {upper_ci})"))
}

base_stats <- list(total = clean_reviews |> distinct(name, specialty) |> nrow(),
                   orthopaedics = clean_reviews |> filter(specialty == "Orthopaedics") |> distinct(name, specialty) |> nrow(),
                   medicine = clean_reviews |> filter(specialty == "Medicine") |> distinct(name, specialty) |> nrow(),
                   reviewers = reviews |> count(reviewer) |> nrow(),
                   english_reviews = reviews |> filter(chart_language == "original") |> count(reviewer) |> nrow(),
                   swedish_reviews = reviews |> filter(chart_language == "Swedish") |> count(reviewer) |> nrow()) |> 
  set_names(\(x) paste0("no_", x)) |> 
  append(list(overall = reviews,
              overall_english = reviews |> filter(chart_language == "original"),
              overall_swedish = reviews |> filter(chart_language == "Swedish")) |> 
           map(\(x) x |> pull(overall) |> bootstrap_ci()) |> 
           set_names(\(x) paste0("score_", x)))
```

```{r Table 1 - full dataset description}
get_median_min_max <- function(x) {
  glue::glue("{median} ({min}&#8209;{max})",
             median = median(x),
             min = min(x),
             max = max(x))
}

raw_with_datasets |> 
  map(\(x) map(x,
               \(chart) {
                 list(name = chart$name,
                      specialty = chart$specialty,
                      language = chart$language,
                      status = chart$status,
                      no_notes = chart$notes |> nrow(),
                      no_medications = chart$medications |> nrow(),
                      no_lab_values = chart$lab_values |> nrow(),
                      length_of_stay = chart$notes |> 
                        summarise(los = as.numeric(difftime(max(date), min(date), units = "days"))) |> 
                        pull(los))
               }) |> 
        bind_rows() |> 
        filter(status != "NOT_FINISHED") |> 
        mutate(language = case_when(language == "original" ~ "English",
                                    TRUE ~ language),
               sort_order = case_when(language == "English" ~ "1",
                                      TRUE ~ language)) |> 
        arrange(sort_order) |> 
        mutate(language = paste(language, collapse = ", ")) |> 
        select(-sort_order) |>
        # The languages don't always match in no. of 
        slice_head(n = 1)) |>
  bind_rows() |> 
  group_by(specialty) |> 
  summarise(`header_Cases` = name |> unique() |> length() |> as.character(),
            header_Languages = language |> unique() |> paste(collapse = ", "),
            `header_Length of stay` = get_median_min_max(length_of_stay),
            `header_No. of notes` = get_median_min_max(no_notes),
            `header_No. of medications` = get_median_min_max(no_medications),
            `header_No. of lab values` = get_median_min_max(no_lab_values),
            .groups = "drop") |> 
  pivot_longer(cols = starts_with("header_"),
               names_to = "header",
               names_prefix = "header_",
               values_to = "value") |> 
  mutate(specialty = case_when(specialty == "Orthopaedics" ~ "Orthopaedic surgery",
                               TRUE ~ specialty),
         cgroup = case_when(startsWith(header, "No.") ~ "Counts (median and range)",
                            TRUE ~ ""),
         header = case_when(startsWith(header, "No.") ~ str_replace(header,
                                                                    "No. of",
                                                                    "") |> 
                              str_trim() |> 
                              Hmisc::capitalize(),
                            TRUE ~ header)) |>
  tidyHtmlTable(rnames = specialty,
                value = value,
                header = header,
                cgroup = cgroup,
                align = "l",
                # Vertical align to top
                css.cell = "vertical-align: top;")
```
**Table 1: Description of the dataset. The table shows the number of cases, the languages used in the cases, the length of stay, the number of notes, medications, and lab values. The values are presented as the median and range.**

# Statistics

Statistical analyses were conducted using R version `r versions$R` and Python `r versions$python`. BLEU scores were computed using the sacrebleu package (version `r versions$sacrebleu`), and ROUGE-L scores were calculated with the rouge package (version `r versions$rouge`) in Python.

Due to the hierarchical nature of the data we used linear mixed effects models. The mixed-effects models were fitted using the lme4 package (version `r versions$lme4`) and broom.mixed (version `r versions$broom.mixed`) in R.

For the manual evaluation outcomes (overall score and domain-specific ratings), the fixed effects in our models included:

- Specialty: orthopedics or internal medicine.
- Language: English or Swedish.
- Generator type: human-written or LLM-generated summaries.
- Interaction terms: between specialty, language, and generator type to assess whether the effect of the generator differed by specialty and language.

We included reviewer identity as a random effect to account for variability among reviewers since each reviewer evaluated multiple summaries (ranging from `r reviews |> count(reviewer) |> pull(n) |> range() |> paste(collapse = " to ")` summaries).

For the automatic evaluation metrics (BLEU and ROUGE-L scores), the fixed effects in our models were:

- Specialty: orthopedics or internal medicine.
- Language: English or Swedish.
- Prompt type: single or multistep prompt.

We included case identity as a random effect to account for variability within cases. Since each case has an English standard text that is translated into multiple languages, including case identity as a random effect helps control for potential similarities within a case.

Model estimates, standard errors, and 95% confidence intervals were calculated for the fixed effects. Statistical significance was assessed at a threshold of p < 0.05.

Bootstrap methods with 1,000 resamples were employed using the boot package (version `r versions$boot`) in R to calculate percentile-based 95% confidence intervals for mean scores. Basic statistical summaries derived from the bootstrapping were presented prior to the advanced analyses to provide an overview of the data distributions. Data visualization was performed using ggplot2 (version `r versions$ggplot2`) and ggpubr (version `r versions$ggpubr`) in R.

No formal model diagnostics were performed for the mixed-effects models. Given that the outcome variables were ratings on a 1–5 scale, we considered linear mixed-effects regression appropriate for this exploratory analysis.

# Results

We evaluated a total of `r base_stats$no_total` unique cases, comprising `r base_stats$no_orthopaedic` orthopedic cases and `r base_stats$no_medicine` internal medicine cases. The orthopedic cases were further evaluated in both English and Swedish, resulting in 15 case-language combinations for which summaries were generated and reviewed. 

The summaries were assessed by `r base_stats$no_reviewers` medical professionals, resulting in a total of `r nrow(reviews)` ratings. For English summaries, `r base_stats$no_english_reviews` eviewers provided evaluations, while `r base_stats$no_swedish_reviews` reviewers assessed the Swedish summaries. The overall mean rating across all summaries was `r base_stats$score_overall`. The average rating for English summaries was `r base_stats$score_overall_english`, whereas Swedish summaries received an average rating of `r base_stats$score_overall_swedish`.

```{python python-metrics, echo=FALSE}
from dataclasses import dataclass, asdict
import pandas as pd
from rouge import Rouge
from sacrebleu.metrics import BLEU

# At first run do this:
# import nltk
# nltk.download('punkt')

df = r.summaries

rouge = Rouge(metrics=['rouge-l'],
              max_n=2,
              limit_length=True,
              length_limit=100,
              length_limit_type='words',
              apply_avg=False,
              apply_best=False,
              alpha=0.5,  # Default F1_score
              stemming=True)
blue = BLEU(effective_order=True)

# Define the data structure for each result row
@dataclass
class SummaryResult:
    specialty: str
    name: str
    chart_language: str
    generatedBy: str
    rouge_l_f: float
    rouge_l_p: float
    rouge_l_r: float
    bleu_score: float
    bleu_bp: float

# Function to calculate scores and return a dataclass instance
def calculate_scores_and_store_results(
    human_summary: str, 
    generated_summary: str, 
    specialty: str, 
    name: str, 
    language: str, 
    generatedBy: str
) -> SummaryResult:
    # Compute ROUGE scores
    score = rouge.get_scores(generated_summary, human_summary)
    rouge_l = score['rouge-l'][0]
    
    # Compute BLEU scores
    bleu_score = blue.sentence_score(generated_summary, [human_summary])

    # Return an instance of SummaryResult dataclass
    return SummaryResult(
        specialty=specialty,
        name=name,
        chart_language=language,
        generatedBy=generatedBy,
        rouge_l_f=rouge_l['f'][0],
        rouge_l_p=rouge_l['p'][0],
        rouge_l_r=rouge_l['r'][0],
        bleu_score=bleu_score.score,
        bleu_bp=bleu_score.bp,
    )

# Main function to process the DataFrame
def retrieve_summary_text_scores(df: pd.DataFrame) -> pd.DataFrame:
    results: list[SummaryResult] = []

    # Iterate using groupby to avoid unnecessary nested loops
    for (specialty, language, name), df_case in df.groupby(['specialty', 'chart_language', 'name']):
        # Get the human summary
        human_summary_row = df_case[df_case['generatedBy'] == 'Human']
        if human_summary_row.empty:
            continue  # Skip if no human summary is available

        human_summary = human_summary_row['summary'].values[0]

        # Get the generated summaries (all except 'Human')
        generated_summaries = df_case[df_case['generatedBy'] != 'Human']

        for idx, row in generated_summaries.iterrows():
            # Calculate scores and store the result as a dataclass instance
            result = calculate_scores_and_store_results(
                human_summary=human_summary, 
                generated_summary=row['summary'],
                specialty=specialty, 
                name=name, 
                language=language, 
                generatedBy=row['generatedBy']
            )
            results.append(result)

    # Convert list of dataclass instances to DataFrame
    return pd.DataFrame([asdict(result) for result in results])

# Convert results to DataFrame
results_df = retrieve_summary_text_scores(df)
```

```{r metrics-regression}
df <- py$results_df |> 
  fix_labels() |> 
  rename(language = chart_language)

# Data preparation using the built-in pipe and across
regression_df <- df |>
  mutate(across(c(specialty, language, prompt_type), as.factor),
         case_id = interaction(specialty, name))  # Create unique case_id for each specialty + name

# Fit a mixed model for rouge_l_f with random intercept for case_id
metrics_regression_outputs <- local({
  fn <- \(x) mutate(x,
                    term = case_when(
                      term == "(Intercept)" ~ "(Intercept) [Reference: Medicine, English, Multistep]",
                      term == "specialtyOrthopaedics" ~ "Orthopaedics [vs. Medicine]",
                      term == "languageSwedish" ~ "Swedish [vs. English]",
                      term == "prompt_typeSingel" ~ "Singel [vs. Multistep]",
                      TRUE ~ term
                    ))
  list(
    rouge = lmer(rouge_l_f ~ specialty + language + prompt_type + (1 | case_id), data = regression_df) |> 
      output_mixed_model("ROUGE-L F1",
                         summary_fn = fn,
                         digits = 2),
    bleu = lmer(bleu_score ~ specialty + language + prompt_type + (1 | case_id), data = regression_df) |> 
      output_mixed_model("BLEU",
                         summary_fn = fn,
                         digits = 1))
})
```

```{r overall_output_regression}
simple_overall <- reviews |> 
  fix_labels() |> 
  fix_factors() |> 
  select(name, specialty, prompt_type, language = chart_language, reviewer, overall) |> 
  lmer(overall ~ specialty + language + prompt_type + (1 | reviewer), data = _) |> 
  output_mixed_model(outcome = "overall score",
                     summary_fn = \(x) mutate(x,
                                              term = case_when(
                                                term == "(Intercept)" ~ "(Intercept) [Reference: Orthopaedics, English, Human]",
                                                term == "specialtyOrthopaedics" ~ "Orthopaedics [vs. Medicine]",
                                                term == "specialtyMedicine" ~ "Medicine [vs. Orthopaedics]",
                                                term == "languageSwedish" ~ "Swedish [vs. English]",
                                                term == "prompt_typeSingel" ~ "Singel [vs. Human]",
                                                term == "prompt_typeMultistep" ~ "Multistep [vs. Human]",
                                                term == "generatorLLM" ~ "LLM [vs. Human]",
                                                term == "specialtyMedicine:generatorLLM" ~ "Interaction: Medicine & LLM",
                                                term == "languageSwedish:generatorLLM" ~ "Interaction: Swedish & LLM",
                                                TRUE ~ term
                                              )))

overall_output <- reviews |> 
  fix_labels() |> 
  fix_factors() |> 
  select(name, specialty, generator, language = chart_language, reviewer, overall) |> 
  lmer(overall ~ (specialty + language) * generator + (1 | reviewer), data = _) |> 
  output_mixed_model(outcome = "overall score",
                     summary_fn = \(x) mutate(x,
                                              term = case_when(
                                                term == "(Intercept)" ~ "(Intercept) [Reference: Orthopaedics, English, Human]",
                                                term == "specialtyOrthopaedics" ~ "Orthopaedics [vs. Medicine]",
                                                term == "specialtyMedicine" ~ "Medicine [vs. Orthopaedics]",
                                                term == "languageSwedish" ~ "Swedish [vs. English]",
                                                term == "prompt_typeSingel" ~ "Singel [vs. Human]",
                                                term == "prompt_typeMultistep" ~ "Multistep [vs. Human]",
                                                term == "generatorLLM" ~ "LLM [vs. Human]",
                                                term == "specialtyMedicine:generatorLLM" ~ "Interaction: Medicine & LLM",
                                                term == "languageSwedish:generatorLLM" ~ "Interaction: Swedish & LLM",
                                                TRUE ~ term
                                              )))
```

## Overall performance

For the overall rating of the summaries, the majority of those deemed unusable were generated by the LLM, while most human-written summaries were regarded as average or above (Figure 1). Using linear mixed-effects regression to model the overall scores, we found that LLM-generated summaries scored significantly lower than human-written summaries by an average of `r toString(overall_output, "LLM [vs. Human]", invert = TRUE, unit = "points")`, see Table 3. 

An interaction effect revealed that LLM-generated summaries in Swedish had an additional decrease of `r toString(overall_output, "Interaction: Swedish & LLM", invert = TRUE, unit = "points")` compared to English summaries. Similarly, LLM-generated summaries for internal medicine cases showed a further decrease of `r toString(overall_output, "Interaction: Medicine & LLM", unit = "points", invert = TRUE)`, although this did not reach statistical significance.

When comparing prompt strategies in a simplified mixed-effects model without interactions, we found that the multistep prompt performed worse than the single-step approach, with a decrease of `r toString(simple_overall, "Multistep [vs. Human]", unit = "points")` versus a decrease of `r toString(simple_overall, "Singel [vs. Human]", unit = "points")` for the single prompt.

## Automatic evaluation metrics

In addition to manual evaluations, we assessed the generated summaries using automatic evaluation metrics (Table 3), specifically BLEU and ROUGE-L scores, to quantify the similarity between the LLM-generated summaries and the reference human-written summaries. The mixed-effects regression analysis for the ROUGE-L F1 score revealed that LLM-generated summaries in Swedish had significantly lower scores compared to those in English, with a decrease of `r toString(metrics_regression_outputs$rouge, "Swedish [vs. English]", invert = TRUE)`. Similarly, for the BLEU score, LLM-generated summaries in Swedish scored significantly lower than their English counterparts, with a reduction of `r toString(metrics_regression_outputs$bleu, "Swedish [vs. English]", invert = TRUE)`.

```{r overall_grap, fig.cap="Figure 1: Overall rating of the summaries by chart language and prompt type."}
colors <- c("Human" = "#7fbf7b", "Multistep" = "#af8dc3", "Singel" = "#e7d4e8")
clean_reviews |> 
  select(chart_language, prompt_type, overall) |> 
  mutate(overall = factor(overall, 
                          labels = c("Unusable",
                                     "Subhuman",
                                     "Average",
                                     "Good",
                                     "Superhuman"))) |>
  ggplot(aes(x = overall, fill = prompt_type)) +
  facet_wrap(~chart_language, ncol = 1) +
  geom_bar(position = "dodge", alpha = 0.8, col = "black") +
  labs(x = "Chart Language",
       y = "Overall Rating") +
  theme_minimal() +
  theme(legend.position = "right",
        strip.text = element_text(size = 12)) +
  scale_fill_manual(values = colors,
                    name = "Prompt Type")
```

```{r overall_mixed_model_output}
print(overall_output)
```

```{r metrics_mixed_model_output, include=FALSE}
print(metrics_regression_outputs$rouge)
print(metrics_regression_outputs$bleu)
```

```{r basic-rouge table}
df |> 
  group_by(language, prompt_type) |>
  summarise(across(starts_with("rouge") | starts_with("bleu"), mean),
            .groups = "drop") |> 
  pivot_longer(cols = -c(language, prompt_type), names_to = "metric", values_to = "value") |> 
  mutate(type = case_when(str_detect(metric, "rouge") ~ "ROUGE",
                          str_detect(metric, "bleu") ~ "BLEU"),
         value = txtRound(value, digits = 2)) |>
  filter(metric %in% c("rouge_l_f", "bleu_score")) |>
  arrange(language, type, prompt_type) |>
  tidyHtmlTable(value = value,
                rnames = prompt_type,
                rgroup = type,
                header = language,
                caption = "ROUGE-L F1 and BLEU scores for generated summaries per language and type")
```

Analysis of specific domains within the summaries revealed that LLM-generated summaries generally performed worse in the follow-up sections compared to human-written summaries (Figure 2). This was particularly evident in the Swedish summaries. In the medications domain, both human and LLM-generated summaries scored lower. For human-written orthopedic discharge summaries, medication information was often omitted, as it is not always standard practice to include detailed medication lists in these documents.

```{r}
prepped_data <- clean_reviews |> 
  pivot_longer(cols = where(is.integer), names_to = "metric", values_to = "value") |>
  group_by(chart_language, prompt_type, metric) |>
  summarise(value = mean(value, na.rm =TRUE), .groups = "drop") |> 
  mutate(metric = case_when(metric == "hospitalCourse" ~ "Hospital course",
                            metric == "medicalHistory" ~ "Medical history",
                            metric == "followUp" ~ "Follow-up",
                            TRUE ~ Hmisc::capitalize(metric))) |> 
  pivot_wider(names_from = metric, values_from = value)
```

```{r radar_data}
max_min <- prepped_data |> 
  select(where(is.numeric)) |> 
  apply(2, function(x) c(4, 1)) |> 
  as.data.frame()

# Combine max, min, and actual data for radar chart
radar_data <- bind_rows(max_min, 
                        prepped_data)
```

```{r radar_plot_helpers}
plot_radar <- function(x, language) {
  data <- x |> 
    filter(chart_language == language | is.na(chart_language)) |> 
    mutate(order = case_when(is.na(prompt_type) ~ 1,
                             prompt_type == "Human" ~ 2,
                             TRUE ~ 10)) |> 
    arrange(order) |>
    select(-order, -chart_language)
    
  legends <- data$prompt_type |> 
    na.omit() |> 
    unique()
  
  main_colors <- RColorBrewer::brewer.pal(8, "Set2")[1:length(legends)] |>
    col2rgb() |> 
    t()
  colors_border <- rgb(main_colors, alpha = 0.9*255, maxColorValue = 255)
  colors_in <- rgb(main_colors, alpha = 0.4*255, maxColorValue = 255)
  
  data |> 
    select(-prompt_type) |> 
    radarchart(axistype = 1,
               maxmin = TRUE,
               # Customize the polygon colors
               pcol = colors_border,
               pfcol = colors_in,
               plwd = 4,
               # Customize the grid colors
               cglcol = "grey", 
               cglty = 1, 
               axislabcol = "grey", 
               caxislabels = seq(0, 5, 1), 
               cglwd = 0.8,
               # Customize labels
               vlcex = 0.8,
               title = language,
    )
  
  par(xpd = TRUE)
  legend(c(0.5,1), 
         legend = legends, 
         col = colors_border,
         pch = 20, 
         bty = "n",
         cex = 0.8)
}

per_language_radar <- function(x) {
  languages <- x$chart_language |> 
    na.omit() |> 
    unique()
  
  # Split the screen into multiple columns (one per language)
  org_par <- par(mfrow = c(1, length(languages)),
                 mar = rep(0.8, 4),  # Outer margins for the overall plot space
                 oma = rep(0.2, 4))  # Outer margins for the overall plot space
  on.exit(par(org_par))  # Reset to original layout
  
  for (language in languages) {
    plot_radar(x, language)
  }
}
```

## Medical accuracy

```{r radar_plot_medical_accuracy, fig.height=6, fig.width=16}
radar_data |> 
  select(chart_language, 
         prompt_type, 
         Diagnosis,
         `Medical history`,
         `Hospital course`,
         `Follow-up`) |> 
  per_language_radar()
```

## Linguistic qualities

```{r radar_linguistics_medical_accuracy, fig.height=6, fig.width=16}
radar_data |> 
  select(chart_language, 
         prompt_type, 
         Conciseness,
         Completeness,
         Language,
         Clarity) |> 
  per_language_radar()
```

# Dot chart

```{r dot_chart, fig.height=6, fig.width=9, fig.cap="Average scores for each metric by prompt type and language."}
prepped_data |> 
  select(-Overall) |> 
  pivot_longer(cols = where(is.numeric), 
               names_to = "metric", 
               values_to = "value") |> 
  ggdotchart(x = "metric", 
             y = "value", 
             color = "prompt_type",
             group = "prompt_type",
             data = _, 
             dot.size = 3, 
             position = position_dodge(0.5),
             add = "segment",
             sort = "ascending",
             ylab = "Average score",
             xlab = "Metric") +
  scale_color_manual(values = colors,
                     name = "Prompt type") +
  facet_grid(chart_language~.) +
  guides(color = guide_legend(override.aes = list(size = 5)))  +
  theme(text = element_text(size = 14),
        # Rotate x-axis labels
        axis.text.x = element_text(angle = 45, hjust = 0.5),
        strip.text = element_text(size = 16, face = "bold"),
        # Increase the labels
        axis.title = element_text(size = 18, face = "bold"),
        legend.title = element_text(size = 14),
        legend.position = "top")
```


# Hallucinations

Hallucinations, defined as instances where the LLM introduced information not present in the source material, were more prevalent in the LLM-generated summaries (Figure 3). These hallucinations frequently appeared in the follow-up plans, where the LLM added recommendations or instructions not included in the original patient records or prompt instructions. For example, the LLM might include advice such as, "In case of suspected complications, the patient should contact the emergency department or the orthopedic outpatient clinic," which was neither part of the provided information nor the prompt instructions. This issue was observed in both orthopedic and internal medicine cases.

```{r}
reviews |> 
  fix_labels() |> 
  select(prompt_type, hallucinations, chart_language) |> 
  mutate(hallucinations = factor(hallucinations,
                                 levels = 0:3,
                                 labels = c("None", 
                                            "1 to 2",
                                            "3 to 4",
                                            "More than 4"))) |> 
  ggplot(aes(x = hallucinations, fill = prompt_type)) +
  facet_grid(~chart_language) +
  geom_bar(position = "dodge", color = "black", alpha = 0.8) +
  scale_fill_manual(values = colors,
                    name = "Prompt Type") +
  labs(title = "Hallucinations in generated summaries",
       x = "Number of hallucinations",
       y = "Count") +
  theme_minimal() +
  theme(legend.position = "right")
```

```{r hallucinations_comments}
reviews |> 
  fix_labels() |> 
  select(prompt_type, specialty, name, hallucinationsComment) |> 
  filter(hallucinationsComment != "") |> 
  mutate(header = "Comment",
         name = interaction(specialty, name, sep = " - "),
         rnames_unique = 1:n()) |> 
  arrange(prompt_type, name)  |> 
  htmlTable::addHtmlTableStyle(col.rgroup = c("none", "#F7F7F7")) |> 
  tidyHtmlTable(tspanner = prompt_type,
                value = hallucinationsComment,
                header = header,
                rnames_unique = rnames_unique,
                align = "l")
```

# General comments

```{r}
reviews |> 
  fix_labels() |> 
  select(prompt_type, specialty, name, overallComment) |> 
  filter(overallComment != "") |> 
  mutate(header = "Comment",
         name = interaction(specialty, name, sep = " - "),
         rnames_unique = 1:n()) |> 
  arrange(prompt_type, name)  |> 
  htmlTable::addHtmlTableStyle(col.rgroup = c("none", "#F7F7F7")) |> 
  tidyHtmlTable(tspanner = prompt_type,
                value = overallComment,
                header = header,
                rnames_unique = rnames_unique,
                align = "l")

```

# Author contributions

```{r}
reviews |> 
  fix_labels() |> 
  group_by(reviewer) |>
  summarise(n = n(),
            .groups = "drop") |>
  arrange(desc(n)) |>
  htmlTable::htmlTable(align = "llr",
                       rnames = FALSE)
```

```{r}
people <- tribble(
  ~id, ~first_name, ~middle_name, ~last_name,
  "clytbas6v00014mzwgjotozwr", "Admin", "", "User",
  "clytbe5d002peyjw71gvj6shv", "Max", "", "Gordon",
  "clz1ey0ub00a2hozvt5lmzyeq", "Julius", "", "Krüger",
  "clz5yw2cc00m7hozvpveci0tp", "Cyrus", "David", "Broden",
  "cm02f4a01002s2dneftyr50ad", "Amir", "H.", "Payberah",
  "cm06n0mbi01iq2dne9mp89iqg", "Robert", "", "Dahlbäck",
  "cm0mwg25y00q1c2jodxw9kry2", "Miguel", "", "Márquez Gómez",
  "cm0qgq13400q4c2jof9g7a4et", "Tazio", "", "Maleitzke",
  "cm0uscb21000sx863n34ynwci", "Giulio Edoardo", "", "Vigni",
  "cm0w1cr5q000v5dp6f8h9vw1d", "Martin", "", "Magnéli",
  "cm0w1dxwf00115dp6gvtljj0u", "Niklas", "Johan Gunnar", "Barle",
  "cm0wxfd9v00655dp66gy3uk8b", "Arnar", "", "Bender",
  "clzyniqi1002j2dners150kpc", "Aleksi", "", "Reito",
  "cm0y74mn3000q5f600niyxobw", "Dana", "", "Khalil",
  "cm0yy02jj000zl5mzk77r0091", "Isaac", "R.", "Romanus",
  "cm17ju72k004a131zbteqrmzf", "Daniel", "", "Lim",
  "cm1ae82rb007d131z3oib4iow", "Viktor", "", "Schmidt",
  "cm1c3g2nx008b131zcfqaojcw", "Thomas", "", "Tamras",
  "cm1dykp2u009q131z2j0eo80j", "Daniela", "", "Gordon"
)

raw_with_datasets |> 
  map(\(x) map(x, \(chart) list(specialty = chart$specialty, 
                                name = chart$name,
                                language = chart$language,
                                createdBy = chart$createdBy)) |> 
        bind_rows()) |>
  bind_rows() |> 
  count(createdBy, specialty, language) |> 
  left_join(people, by = c("createdBy" = "id")) |> 
  filter(first_name != "Admin") |> 
  mutate(name = if_else(is.na(middle_name) | middle_name == "", 
                        paste(first_name, last_name),
                        paste(first_name, middle_name, last_name)),
         name = if_else(name == "Max Gordon",
                        "Aleksi Reito",
                        name)) |> 
  select(name, specialty, language, n) |> 
  htmlTable(caption = "Number of charts created by each user. Note that users that have submitted charts using word/excel are not included here.",
            align = "lllr",
            rnames = FALSE)
```

